{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RoBERTa"
      ],
      "metadata": {
        "id": "COE-KEw3I2Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 드라이브 연동\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJrNHtTpO4XG",
        "outputId": "ab9d8f1a-260e-4998-e6db-9a88d5af6f17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 민서가 준 data.csv 파일"
      ],
      "metadata": {
        "id": "B2H4JMeGRkG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data 변수명으로 csv 파일 열기\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/BITAMIN/24 nlp [projects]/주제별 카톡(1)/data.csv')\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns6WBtb1RiUb",
        "outputId": "f73c6917-c6ea-412e-ff95-a2f3f4a5c075"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      subject                                               text\n",
            "0     타 국가 이슈  1 : 인도는 폭우가 와서 난리라네\\n2 : 요새 그냥 지구가 난리다 ㅠㅠ\\n3 :...\n",
            "1          미용  1 : 다이어트가 최고의 성형이야!\\n2 : 다이어트가 최고의 성형이지 하하\\n3 ...\n",
            "2          건강  1 : 건강 해야 해 친구들 우리 모두\\n2 : 맞아 건강해야 해 하하\\n3 : 그...\n",
            "3      상거래 전반  1 : 요새 뭐가 젤 갖고 싶누 하하\\n2 : 나는 루이비통 후드티 하하\\n3 : ...\n",
            "4       방송/연예  1 : 아 그거 기대작임 지리산\\n2 : 지리산 이번 주에 함 하하\\n2 : 완전 ...\n",
            "...       ...                                                ...\n",
            "4994   스포츠/레저  1 : 나 오늘부터 헬스 다닐거야!\\n2 : 오 너 꾸준히 다닐 수 있음?\\n1 :...\n",
            "4995       게임  1 : 나 요즘 카트라이더가 왜 이리 재밌는지 모르겠어 하하\\n2 : 너도 카트라이...\n",
            "4996       여행  1 : 요즘 따라 너무 일본이 가고 싶어 ㅠㅠ\\n2 : 너 일본 한 번도 안 가 봄...\n",
            "4997    계절/날씨  1 : 나 어제 강릉 갔다 온 거 말했었나? 하하\\n2 : 엉? 너 어제 강릉갔다 ...\n",
            "4998     사회이슈  1 : 너 하루에 커피 몇 잔 마셔?\\n2 : 나는 커피 안 좋아함.\\n1 : 오 ...\n",
            "\n",
            "[4999 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(data) # 데이터 프레임"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "nOuARcvDJPjG",
        "outputId": "4261a747-4559-4b55-cb33-901f9ca89d43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.frame.DataFrame</b><br/>def __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py</a>Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n",
              "\n",
              "Data structure also contains labeled axes (rows and columns).\n",
              "Arithmetic operations align on both row and column labels. Can be\n",
              "thought of as a dict-like container for Series objects. The primary\n",
              "pandas data structure.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n",
              "    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n",
              "    data is a dict, column order follows insertion-order. If a dict contains Series\n",
              "    which have an index defined, it is aligned by its index. This alignment also\n",
              "    occurs if data is a Series or a DataFrame itself. Alignment is done on\n",
              "    Series/DataFrame inputs.\n",
              "\n",
              "    If data is a list of dicts, column order follows insertion-order.\n",
              "\n",
              "index : Index or array-like\n",
              "    Index to use for resulting frame. Will default to RangeIndex if\n",
              "    no indexing information part of input data and no index provided.\n",
              "columns : Index or array-like\n",
              "    Column labels to use for resulting frame when data does not have them,\n",
              "    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n",
              "    will perform column selection instead.\n",
              "dtype : dtype, default None\n",
              "    Data type to force. Only a single dtype is allowed. If None, infer.\n",
              "copy : bool or None, default None\n",
              "    Copy data from inputs.\n",
              "    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n",
              "    or 2d ndarray input, the default of None behaves like ``copy=False``.\n",
              "    If data is a dict containing one or more Series (possibly of different dtypes),\n",
              "    ``copy=False`` will ensure that these inputs are not copied.\n",
              "\n",
              "    .. versionchanged:: 1.3.0\n",
              "\n",
              "See Also\n",
              "--------\n",
              "DataFrame.from_records : Constructor from tuples, also record arrays.\n",
              "DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n",
              "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
              "read_table : Read general delimited file into DataFrame.\n",
              "read_clipboard : Read text from clipboard into DataFrame.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Please reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing DataFrame from a dictionary.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [1, 2], &#x27;col2&#x27;: [3, 4]}\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d)\n",
              "&gt;&gt;&gt; df\n",
              "   col1  col2\n",
              "0     1     3\n",
              "1     2     4\n",
              "\n",
              "Notice that the inferred dtype is int64.\n",
              "\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int64\n",
              "col2    int64\n",
              "dtype: object\n",
              "\n",
              "To enforce a single dtype:\n",
              "\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int8\n",
              "col2    int8\n",
              "dtype: object\n",
              "\n",
              "Constructing DataFrame from a dictionary including Series:\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [0, 1, 2, 3], &#x27;col2&#x27;: pd.Series([2, 3], index=[2, 3])}\n",
              "&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n",
              "   col1  col2\n",
              "0     0   NaN\n",
              "1     1   NaN\n",
              "2     2   2.0\n",
              "3     3   3.0\n",
              "\n",
              "Constructing DataFrame from numpy ndarray:\n",
              "\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n",
              "...                    columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; df2\n",
              "   a  b  c\n",
              "0  1  2  3\n",
              "1  4  5  6\n",
              "2  7  8  9\n",
              "\n",
              "Constructing DataFrame from a numpy ndarray that has labeled columns:\n",
              "\n",
              "&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n",
              "...                 dtype=[(&quot;a&quot;, &quot;i4&quot;), (&quot;b&quot;, &quot;i4&quot;), (&quot;c&quot;, &quot;i4&quot;)])\n",
              "&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=[&#x27;c&#x27;, &#x27;a&#x27;])\n",
              "...\n",
              "&gt;&gt;&gt; df3\n",
              "   c  a\n",
              "0  3  1\n",
              "1  6  4\n",
              "2  9  7\n",
              "\n",
              "Constructing DataFrame from dataclass:\n",
              "\n",
              "&gt;&gt;&gt; from dataclasses import make_dataclass\n",
              "&gt;&gt;&gt; Point = make_dataclass(&quot;Point&quot;, [(&quot;x&quot;, int), (&quot;y&quot;, int)])\n",
              "&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n",
              "   x  y\n",
              "0  0  0\n",
              "1  0  3\n",
              "2  2  3\n",
              "\n",
              "Constructing DataFrame from Series/DataFrame:\n",
              "\n",
              "&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df\n",
              "   0\n",
              "a  1\n",
              "c  3\n",
              "\n",
              "&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], columns=[&quot;x&quot;])\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df2\n",
              "   x\n",
              "a  1\n",
              "c  3</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 509);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gtWsut3gJjG7",
        "outputId": "b130ea52-5fb7-4282-b581-df8dc065fe73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   subject                                               text\n",
              "0  타 국가 이슈  1 : 인도는 폭우가 와서 난리라네\\n2 : 요새 그냥 지구가 난리다 ㅠㅠ\\n3 :...\n",
              "1       미용  1 : 다이어트가 최고의 성형이야!\\n2 : 다이어트가 최고의 성형이지 하하\\n3 ...\n",
              "2       건강  1 : 건강 해야 해 친구들 우리 모두\\n2 : 맞아 건강해야 해 하하\\n3 : 그...\n",
              "3   상거래 전반  1 : 요새 뭐가 젤 갖고 싶누 하하\\n2 : 나는 루이비통 후드티 하하\\n3 : ...\n",
              "4    방송/연예  1 : 아 그거 기대작임 지리산\\n2 : 지리산 이번 주에 함 하하\\n2 : 완전 ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9757312d-7dd9-460b-bc04-aab08f19636b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>타 국가 이슈</td>\n",
              "      <td>1 : 인도는 폭우가 와서 난리라네\\n2 : 요새 그냥 지구가 난리다 ㅠㅠ\\n3 :...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>미용</td>\n",
              "      <td>1 : 다이어트가 최고의 성형이야!\\n2 : 다이어트가 최고의 성형이지 하하\\n3 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>건강</td>\n",
              "      <td>1 : 건강 해야 해 친구들 우리 모두\\n2 : 맞아 건강해야 해 하하\\n3 : 그...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>상거래 전반</td>\n",
              "      <td>1 : 요새 뭐가 젤 갖고 싶누 하하\\n2 : 나는 루이비통 후드티 하하\\n3 : ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>방송/연예</td>\n",
              "      <td>1 : 아 그거 기대작임 지리산\\n2 : 지리산 이번 주에 함 하하\\n2 : 완전 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9757312d-7dd9-460b-bc04-aab08f19636b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9757312d-7dd9-460b-bc04-aab08f19636b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9757312d-7dd9-460b-bc04-aab08f19636b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6c2e89e3-c239-4115-9d9f-2e5ab27062bd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6c2e89e3-c239-4115-9d9f-2e5ab27062bd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6c2e89e3-c239-4115-9d9f-2e5ab27062bd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 4999,\n  \"fields\": [\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"\\ud0c0 \\uad6d\\uac00 \\uc774\\uc288\",\n          \"\\uc5ec\\ud589\",\n          \"\\uc2a4\\ud3ec\\uce20/\\ub808\\uc800\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4999,\n        \"samples\": [\n          \"1 : \\ub108\\ub124 \\uc1fc\\ubbf8 \\uc548 \\ubcf4\\uc9c0?\\n2 : \\ub098\\ub294 \\uc548 \\ubd10\\uc720 \\u315c\\u315c\\n2 : \\uc7ac\\ubc0c\\ub098?\\n3 : \\uc1fc\\ubbf8 \\ubcf4\\ub294\\ub370 \\uc9c4\\uc9dc \\uc7ac\\ubc0c\\uc5b4\\n1 : \\uc1fc\\ubbf8\\uac00 \\uc694\\uc998 \\ub300\\uc138\\uc58c...\\n2 : \\uc544 \\uadf8\\uac70 \\ub178\\ub798\\ub294 \\ub4e4\\uc5c8\\ub294\\ub370 \\ud0a4\\ud0a4\\n3 : \\ub108\\ud76c\\ub4e4\\ub3c4 \\ubd10\\ubd10\\n3 : \\uc644\\uc83c \\uc7ac\\ubc0c\\uc5b4 \\ud0a4\\ud0a4\\n1 : \\ud5d0 *\\uc774 \\ubcf4\\ub294\\uad6c\\ub098?\\n1 : \\ub09c \\ucfe4\\ud0c0 \\ud32c\\uc774\\uc57c...\\n2 : \\ub8e8\\uc774\\ube44\\ud1b5 \\ud558\\ub294 \\uadf8 \\ub178\\ub798 \\ud0a4\\ud0a4\\n3 : \\ucfe4\\ud0c0 \\ub7a9 \\ub108\\ubb34 \\uc798\\ud574 \\uc9c4\\uc9dc \\ud0a4\\ud0a4\\n1 : \\ucfe4\\ud0c0 \\uc9c4\\uc9dc \\ubbf8\\uce5c \\uc0ac\\ub78c \\uac19\\uc560\\n2 : \\uadf8 \\ub178\\ub798\\ub294 \\uadfc\\ub370 \\uc9c4\\uc9dc \\uc88b\\ub354\\ub77c\\n3 : \\uc544\\ub9c8 \\uc6b0\\uc2b9 \\ud6c4\\ubcf4\\uc77c \\uac83 \\uac19\\uc544 \\ud0a4\\ud0a4\\n1 : \\ube44\\uc624 \\ub9d0\\ud558\\ub294 \\ub4ef \\ud0a4\\ud0a4\\n1 : \\uadf8 \\ub178\\ub798 \\uc911\\ub3c5\\uc784\\n2 : \\ucfe4\\ud0c0\\uac00 \\ub204\\uad6c\\uc9d5...\\n2 : \\ub098\\ub9cc \\ubab0\\ub77c \\u315c\\u315c\\n3 : \\ube44\\uc624\\ub294 \\uc778\\uae30 \\uc808\\uc815\\uc774\\uc57c \\uc694\\uc998\\n1 : \\uc798 \\ub54c \\uacc4\\uc18d \\uc0dd\\uac01\\ub098 \\ud0a4\\ud0a4\\n2 : \\uc9c4\\uc9dc \\uadf8 \\ub178\\ub798 \\uc911\\ub3c5\\uc131 \\uc788\\uc5b4 \\ud0a4\\ud0a4\\n3 : \\ubb34\\ud55c \\ubc18\\ubcf5 \\uc911\\uc774\\uc57c \\ud0a4\\ud0a4\\n3 : \\ube44\\uc624 \\ub178\\ub798\",\n          \"1 : \\uacf5\\uad70 \\ud6c8\\ub828 \\ube44\\ud589\\ub2e8 \\uc9c4\\uc9dc \\uc2dc\\ub044\\ub7fd\\n2 : \\uc544 \\ub204\\ub098 \\ubc14\\ub85c \\uc606\\uc774\\uc9c0?\\n2 : \\ub9e4\\uc77c \\ube44\\ud589 \\ud558\\ub294 \\uac70 \\ub54c\\ubb38\\uc5d0 \\uc2dc\\ub044\\ub7fd\\uaca0\\ub2e4\\n1 : \\uc5b4\\uc81c \\uacbd\\ube44\\ud589\\uae30 \\ucd94\\ub77d\\ud574\\uc11c \\ub354 \\uc2e0\\uacbd \\uc4f0\\uc784\\n2 : ? \\ucd94\\ub77d\\uc744 \\ud588\\ub2e4\\uace0?\\n2 : \\ubbfc\\uac00\\uc5d0 \\ub5a8\\uc5b4\\uc9c0\\uace0 \\uadf8\\ub7f0 \\uac70 \\uc544\\ub2c8\\uc81c?\\n1 : \\uc5c9 \\uc57c\\uac04 \\ube44\\ud589 \\ud6c8\\ub828\\ub3c4 \\ud55c\\ub2e8\\ub2e4 \\ud6c4\\ud6c4\\ud6c4\\n2 : \\uc544\\ub2c8 \\ubd88\\uc548\\ud574\\uc11c \\uc0b4\\uaca0\\ub098 ?\\n1 : \\uc5c9 \\uce98\\ub9ac\\ud3ec\\ub2c8\\uc544\\uc5d0\\uc11c \\uc8fc\\ud0dd\\uac00\\ub85c \\ucd94\\ub77d\\ud588\\uc5b4!  \\ub09c\\ub9ac\\n2 : \\ucc38... \\uc601\\ud1a0 \\ubc29\\uc704\\ub97c \\uc704\\ud574 \\ud544\\uc694\\ud558\\uc9c0\\ub9cc ...\\n2 : \\uc774\\ub7f0 \\uc0ac\\uace0\\ub294 \\uc5c6\\uae38 \\ubc14\\ub780\\ub2e4\\n1 : \\uc544\\ub2c8 \\uc5c4\\uccad \\ub0ae\\uac8c \\ub0a0\\uc544\\n1 : \\uacbd\\ube44\\ud589\\uae30\\ub77c\\uc11c \\uadf8\\ub7f0\\uac00?\\n2 : \\uac00\\ubcbc\\uc6b4 \\ube44\\ud589\\uae30\\ub77c\\uc11c \\uacbd\\ube44\\ud589\\uae30\\n1 : \\uadf8\\ub9ac\\uace0 \\uc9c0\\uae08 \\ud6c8\\ub828 \\ubc1b\\ub294 \\uc911\\uc778 \\uad70\\uc778\\uc778 \\uac70\\uc796\\uc544\\n2 : \\uc751 \\uad70\\uc778\\uc774\\uc9c0 \\ub2e4\\ub4e4 \\ud0a4\\ud0a4\\n1 : \\uc544 \\uac00\\ubcbc\\uc6cc\\uc11c \\uacbd\\ube44\\ud589\\uae30\\uad6c\\ub098 \\ud0a4\\ud0a4\\n1 : \\uc138\\uc0c1 \\ubb34\\uc2dd\\ud588\\ub124 \\uac00\\ubcbc\\uc6b8 \\uacbd!\\n2 : \\ud0a4\\ud0a4 \\ub098\\ub3c4 \\ud56d\\uacf5\\uc815\\ube44 \\ud558\\uae30 \\uc804\\uae4c\\uc9c0\\ub294 \\uc798 \\ubab0\\ub790\\uc74c\\n1 : \\uc800 \\uc0bc\\ucc9c\\ud3ec \\ucabd\\uc73c\\ub85c \\ub0a0\\uc9c0\\n2 : \\uc2a4\\uc744 \\ub0a0\\uc544\\uc11c \\uae08\\ubc29 \\ub3cc\\uc544 \\uc624\\ub294 \\uac70 \\uac19\\ub358\\ub370 ?\",\n          \"1 : \\uc640 \\ud604\\uc544\\ub294 \\ubab8\\ubb34\\uac8c\\uac00 41kg\\ub798\\n2 : \\ud604\\uc544? \\ub108\\ubb34 \\ub9d0\\ub790\\uc5b4 41KG\\uc740 \\ud0a4\\ud0a4\\n3 : \\ud0a4\\ud0a4 \\uc640 \\uc9c4\\uc9dc \\ub098\\ubcf4\\ub2e4 \\ub35c \\ub098\\uac00\\ub124\\n1 : \\uadf8\\ub7f0 \\ubab8\\uc73c\\ub85c \\ucda4\\uc744 \\ucd94\\ub294 \\uac8c \\uac00\\ub2a5\\ud55c\\uac00...\\n2 : \\uadf8\\ub2c8\\uae4c \\ud0a4\\ud0a4 \\ubf08 \\ubcf4\\uc774\\uace0 \\uadf8\\ub7f0 \\uac70 \\uc544\\ub2c8\\uc9c0...?\\n3 : \\uadf8 \\uc815\\ub3c4\\uba74 \\ubf08\\ub9cc \\uc788\\uc744 \\uac83 \\uac19\\uc740\\ub370,\\n1 : \\uc4f0\\ub7ec\\uc9c8 \\uac83 \\uac19\\uc740\\ub370... \\ud5c8\\ub9ac\\ub3c4 21\\uc778\\uce58\\ub798\\n2 : \\uc9c4\\uc9dc \\ud604\\uc544\\uac00 \\uc778\\uae30 \\uc788\\ub294 \\ub370\\ub294 \\uc774\\uc720\\uac00 \\uc788\\ub124~\\n3 : \\ud604\\uc544 \\ub0a8\\uce5c \\uc774\\ub358\\ub3c4 \\uc5c4\\uccad \\ub9d0\\ub790\\ub300\\n1 : \\ud504\\ub85c \\uc815\\uc2e0\\uc774\\uae34 \\ud55c\\ub370 \\ub108\\ubb34 \\uac71\\uc815\\ub41c\\ub2e4... \\uac74\\uac15\\uc774\\n2 : \\uadf8\\ub7ec\\uac8c \\ud604\\uc544\\ub294 \\uc0b4 \\uc880 \\ub354 \\ucc0c\\uc6cc\\uc57c \\ud574! \\ud558\\ud558\\n3 : \\uc5b4\\ub514\\uc11c \\ubd24\\ub294\\ub370 \\ud3c9\\uc18c\\uc5d0 \\uba39\\ub294 \\uc785\\ub9db\\uc774 \\uc5c6\\ub300\\n1 : \\uc544 \\uc815\\ub9d0?\\n1 : \\ud655\\uc2e4\\ud788 \\uc77c\\ubc18\\uc778\\ub4e4\\uc774\\ub791 \\ub2e4\\ub978\\uac00 \\ubd10 \\u3160\\u3160\\n2 : \\uadf8\\ub807\\uae34 \\ud558\\uc9c0\\n2 : 41KG\\uc740 \\uc77c\\ubc18\\uc778\\ub4e4 \\uc911\\uc5d0 \\ucc3e\\uc544\\ubcf4\\uae30 \\ud798\\ub4e4 \\uac70 \\uac19\\uc544\\n3 : \\ub9de\\uc544 \\ub098\\ub294 \\ub108\\ubb34 \\uc785\\ub9db \\ub3cc\\uc544\\uc11c \\ubb38\\uc81c\\uc778\\ub370 \\ud604\\uc544\\ub294 \\uc785\\ub9db\\uc774 \\uc544\\uc608 \\uc5c6\\ub294 \\uac8c \\ub108\\ubb34 \\uc2e0\\uae30\\ud574 \\ud0a4\\ud0a4\\n1 : \\uadf8\\ub7ec\\uac8c, \\uadf8\\ub9ac\\uace0 \\uc720\\uc9c0\\ub97c \\ud55c\\ub2e4\\ub294 \\uac8c \\uc815\\ub9d0 \\uc27d\\uc9c0 \\uc54a\\uc740\\ub370...\\n2 : \\ub098\\ub294 \\uc720\\uc9c0\\ub3c4 \\ubabb \\ud558\\uace0 \\ube7c\\uc9c0\\ub3c4 \\ubabb \\ud560 \\ubab8\\ubb34\\uac8c\\uc57c \\ud0a4\\ud0a4\\n3 : \\ub9de\\uc544 \\ub098\\ub294 \\uc808\\ub300 \\ubabb\\ud560 \\uac83 \\uac19\\uc544\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "1zml2OH5LSkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RoBERTa는 input으로 '문장'을 받음 -> 텍스트 데이터 토큰화\n",
        "\n",
        "labels = data['subject'].values # 값만 따로 뽑아내기\n",
        "texts = data['text'].values"
      ],
      "metadata": {
        "id": "BobSrR9iJ5Ix"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDxjhFynJ5EG",
        "outputId": "96795f5a-bcfd-47e7-900c-98b2895778e3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['타 국가 이슈', '미용', '건강', ..., '여행', '계절/날씨', '사회이슈'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O_MKQmUBJ4_v",
        "outputId": "2b9ea717-1f9c-49eb-df44-75f50024af09"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['1 : 인도는 폭우가 와서 난리라네\\n2 : 요새 그냥 지구가 난리다 ㅠㅠ\\n3 : 인도는 요새 계속 난리네 ?\\n1 : 사람이 벌써 30명이 죽었대!\\n2 : 우리나라도 그렇고 하하\\n3 : 헐 대박이네\\n1 : 홍수가 나가꾸 ㅠ 난리 난리\\n2 : 요새 왜이리 이상기후 심한 거 같니\\n3 : 얼마나 났길 래 사람이죽어\\n1 : 먼 사람이 그리 많이 죽었을꼬\\n2 : 이러다가 진짜 멸망하는 거 아니냐고\\n3 : 맞아 ㅜㅜ 너무 날씨 미쳤음\\n1 : 거기 홍수 나면 답도 없을 듯\\n2 : 욜로족으로 살아야 되나\\n3 : 우리 살아 있을 때 멸망 안 하길\\n1 : 엉 지구가 아파요다 완전\\n2 : 플라스틱을 줄이라고 하하\\n3 : 그래 ㅜㅜ\\n3 : 우리도 그렇고 다들 그렇때\\n1 : 근데 옛날에도 홍수는 낫지 않아 ?\\n2 : 마지막으로 지구가 주는 경고야 이건\\n3 : 맞지 ㅜㅜ플라스틱진짜 미쳤다\\n3 : 요새 배달도그렇고\\n1 : 지구 온난화가 영향이 있는 건가?\\n2 : 구니까 우리하나하나가 노력해야함\\n3 : 그렇지 않을까 키키',\n",
              "       '1 : 다이어트가 최고의 성형이야!\\n2 : 다이어트가 최고의 성형이지 하하\\n3 : 웅 난 요새 다시 살찌는중 ㅜㅜ\\n1 : **는 꾸준히 노력하더라구\\n2 : ***도 요새 살 빠지고 이뻐짐\\n3 : 응 근데 안 빠짐 키키\\n3 : 비밀인데\\n1 : 저녁 굶고 막 나름 노력해\\n2 : **는 열심히 운동함 하하\\n3 : 오 키키 진짜 키키 조만간 샐러드 파티 해야겠네\\n1 : 키키 오 ** 오빠가 칭찬 해줬어 키키\\n2 : ***랑 같이 하하\\n3 : 키키 아 진짜\\n3 : 같이 하면 좋지\\n1 : ***가 와서 운동은 열심히 해...\\n2 : 꽤나 많이 빠짐 하하\\n3 : 우리는 운동을 같이 안 해\\n1 : 근데 처갓집 넘 빠져서...\\n2 : ***는 근데 안 빠짐 하하\\n3 : 아 처갓집 키키 더 열심히 운동하면 대\\n1 : 처갓집이 지금 다이어트의 적이야 키키\\n2 : 처갓집 꼭 먹어보렴\\n2 : 슈프림 하하\\n3 : 키키 아 그 정도라고\\n3 : 나 안 먹어봄\\n1 : 진짜 왜 ***는 더 쪘지?\\n2 : 그니까\\n2 : 재는 이상하다니까\\n3 : 다이어트를 방해하는 정도로 맛있다고 ?',\n",
              "       '1 : 건강 해야 해 친구들 우리 모두\\n2 : 맞아 건강해야 해 하하\\n3 : 그렇지 요샌 건강이 최고다\\n1 : 운동도 열심히 해야 하구\\n2 : 건강검진도 좀 하고\\n3 : 아프면 진짜 돈도 필요 없고\\n1 : 보험 많이 들어뒀지?\\n2 : 보험 많이 들어났지\\n3 : 응 많지 많아 키키\\n1 : 돈 없으면 또 못 고친다...\\n2 : 요새는 의료보험 잘돼있어서 근데 하하\\n3 : 원래 가난할수록 보험이 많아야해\\n1 : 병을 대비해서 들어둬... 핵아\\n2 : 암 걸려도 다 의료보험 해준다던디 하하\\n3 : 근데 잘 써먹질 못하네\\n1 : 맞아 우린 가난하니까 엄청 들어둠...\\n2 : 그래도 건강이 최고여\\n3 : 그래서 달달이 좀 힘들다 키키\\n3 : 보험료 때매\\n1 : 건강이 최고여!\\n1 : 로또보다 건강!\\n2 : 보험비 안 타도 되니까 건강하자구 모두들 하하\\n3 : 키키 맞아 그게 최고지\\n1 : 맞아 맞아 사고도 항상 조심하구\\n2 : 그래야 좋은 거 많이 보고 살지 하하\\n3 : 웅 차 사고도 조심해야 대고 키키',\n",
              "       ...,\n",
              "       '1 : 요즘 따라 너무 일본이 가고 싶어 ㅠㅠ\\n2 : 너 일본 한 번도 안 가 봄?\\n1 : 아니\\n1 : 한 3번은 가 봤어 하하\\n2 : 근데 일본이 또 가고 싶음?\\n1 : 그 일본 특유의 감성이라 해야 하나? ...\\n1 : 좋더라\\n2 : 키키 맞아\\n2 : 근데 오사카는 그냥 명동 같던데?\\n1 : 그래도 일본어 적혀 있어서 느낌이 다르잖아~\\n2 : 키키 야 명동도 중국어랑 일본어 오져,\\n1 : 그런가?\\n1 : 내가 명동을 안 가 봐서 잘 모르겠네 키키\\n2 : 키키 걍 진짜 오사카랑 판박이야\\n1 : 아닐 거야 그래도 난 일본 가고 싶어 ㅠㅠ\\n1 : 라멘이 먹고 싶거든\\n2 : 아 이치란 라멘 또 먹고 싶네.\\n1 : 그치?\\n1 : 나도 그 라멘이 아직도 기억 나거든\\n2 : 키키 맞아\\n2 : 거기 진짜 존맛이더라.\\n1 : 다음에 일본 가면 무조건 거기부터 갈 거야!\\n2 : 아 인정 키키\\n2 : 나도 니가 말하니까 점심에 라멘 먹어야겠다!',\n",
              "       '1 : 나 어제 강릉 갔다 온 거 말했었나? 하하\\n2 : 엉? 너 어제 강릉갔다 옴?\\n1 : 응 어제 고속도로타고 서울 오는데 깜짝 놀랬어 키키\\n2 : 아 진짜?\\n2 : 왜? 강릉 날씨 좋아서?\\n1 : 아니 대관령 넘어오는데 안개가 진짜 대박이야...\\n1 : 죽을 뻔\\n2 : 헐 대관령, 안개,\\n2 : 나도 보고 싶다!\\n1 : 앞이 안 보여서 너무 위험하던데 ㅠㅠ?\\n2 : 아 그렇게 안 보였어?\\n2 : 비상등 켰고?\\n1 : 당연하지 키키\\n1 : 근데 안개가 너무 짙어서 하나도 안 보였어\\n2 : 키키 헉 그런 날씨에 차사고 나기 딱 좋은데!\\n1 : 그니까 키키\\n1 : 안개가 나는 비보다 더 무서워\\n2 : 맞아 그건 인정 키키\\n2 : 나도 안개 끼면 운전하기 싫더라\\n1 : 안개지역 이제 둘러서 가려고 무섭더라\\n2 : 오우 ㅜㅜ\\n2 : 꼭 안전운전 해야 혀,\\n1 : 그래야지 하하\\n1 : 안개 덕에 좋은 경험 했지 뭐~\\n2 : 헐 그렇게 생각하지 말고 조심히 다녀!\\n2 : 꼭 살아서 보자!',\n",
              "       '1 : 너 하루에 커피 몇 잔 마셔?\\n2 : 나는 커피 안 좋아함.\\n1 : 오 넌 다행이다\\n1 : 난 커피 하루에 한 잔 마시는데 이게 사치래 키키\\n2 : 시롸?\\n2 : 우리나라 사람들 그러면 다 김치임?\\n1 : 뉴스 봤는데 초콜렛이랑 커피가 완전 사치품이라는데?...\\n2 : 엥? 초콜렛이랑 커피가 어떻게 사치품이여!\\n1 : 옛날에 귀족들이 사치품으로 먹었던 거라 하더라 키키\\n2 : 키키 그때랑 지금이랑 같냐 어휴,\\n1 : 미래에도 커피 1잔 값이 더 오를 거래\\n2 : 아 맞아\\n2 : 그 기사는 봄!\\n1 : 응 그래서 미래에는 더욱 사치품으로 바뀔 가능성이 높다더라\\n2 : 아 이제 카카오랑 원두 재배 못한대?\\n1 : 재배량은 떨어지고 찾는 사람은 많아 져서 그런 거 같아\\n2 : 내가 알기론 그것도 코로나 영향이 크다는데\\n1 : 아 진짜?\\n1 : 하긴 뉴스 기사에도 코로나 관련 얘기가 있더라\\n2 : 맞아\\n2 : 코로나가 사람 여럿 죽여 ㅜ'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 일부 데이터만 모델 돌리기\n",
        "labels = data['subject'].head(100).values\n",
        "texts = data['text'].head(100).values"
      ],
      "metadata": {
        "id": "c7AckRNTpPdF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 분할(train, validation)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "vlh8-oG0J49X"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoBERTa 모델링"
      ],
      "metadata": {
        "id": "YNDEgJs2KG37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformers 라이브러리 사용 -> transformers와 torch 라이브러리 설치\n",
        "!pip install transformers torch # 완료"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9pccwkyXJ46w",
        "outputId": "833785d7-d884-4eae-b792-65bb2193697c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face 의 transformeers 라이브러리를 사용해 RoBERTa 전용 토크나이저 만들기\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "# RoBERTa-base 모델의 토크나이저 로드\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# 토큰화\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=128)\n"
      ],
      "metadata": {
        "id": "9i61vH3ZJ42j"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "^ 설명\n",
        "\n",
        "<`from_pretrained` 메서드>\n",
        "- Hugging Face의 `transformers` 라이브러리에서 제공하는 기능으로, 미리 학습된 사전 학습 모델을 불러오는 데 사용된다.\n",
        "- `모델이름('roberta-base')`\n",
        "    - 이 메서드에서 `roberta-base` 모델을 사용해서 미리 학습된 RoBERTa-base 모델을 불러온다. 이 모델을 일반적으로 영어 텍스트로 학습된 사전 학습 모델이다. *(흠.. 그렇다면 한국어로 학습되어있는걸 사용해야하지 않을까?)*\n"
      ],
      "metadata": {
        "id": "ahh8SMN_O_JC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 준비\n",
        "\n",
        "import torch\n",
        "\n",
        "# `TextDataset` 클래스 정의\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# `train_dataset` 및 `val_dataset` 인스턴스 생성\n",
        "train_dataset = TextDataset(train_encodings, train_labels)\n",
        "val_dataset = TextDataset(val_encodings, val_labels)\n"
      ],
      "metadata": {
        "id": "tQgy_2fxJ40N"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "^\n",
        "\n",
        "PyTorch의 `Dataset` 클래스를 사용해 학습과 검증에 사용할 데이터셋 준비.\n",
        "  \n",
        "1. `TextDataset` 클래스 정의\n",
        "- PyTorch의 `Dataset` 클래스를 상속하여 데이터셋을 구성\n",
        "- 이렇게 하면, 모델 학습 시 데이터가 필요할 때마다 하나씩 샘플을 제공하여 효율적으로 데이터를 로드할 수 있음\n",
        "2. `train_dataset` 및 `val_dataset` 인스턴스 생성"
      ],
      "metadata": {
        "id": "s3hho5bORuUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 로드 및 설정\n",
        "# RoBERTa 모델을 불려오고, 텍스트 분류 작업을 위해 분류 헤드를 추가\n",
        "\n",
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "# 2개 이상의 주제나 감정 범주에 따라 num_labels 값을 조정\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuOXUqpcJ4x2",
        "outputId": "5ea01641-e9a0-4f70-d01b-0e74ce5e4034"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 설정 및 훈련\n",
        "# 학습을 위한 `Trainer`와 `TrainingArguments` 설정\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # 출력 폴더\n",
        "    num_train_epochs=3,              # 학습 반복 횟수\n",
        "    per_device_train_batch_size=8,   # 학습 배치 크기\n",
        "    per_device_eval_batch_size=8,    # 평가 배치 크기\n",
        "    warmup_steps=500,                # 워밍업 단계\n",
        "    weight_decay=0.01,               # 가중치 감쇠\n",
        "    logging_dir='./logs',            # 로깅 폴더\n",
        "    report_to=\"none\"                 # W&B 비활성화\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "xCT6iWpdJ4vg",
        "outputId": "5d1a1adc-adcd-4435-febe-546af5ba439e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "new(): invalid data type 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-3c8179e0b5d7>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2237\u001b[0m                 \u001b[0mtotal_batched_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-cb11d0578e9a>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "# 학습이 완료되면 검증 데이터셋을 사용하여 모델을 평가한다.\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n"
      ],
      "metadata": {
        "id": "IPt2kzXKJ4tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 새로운 텍스트에 대한 예측\n",
        "\n",
        "test_text = [\"예시 문장입니다.\"]\n",
        "test_encodings = tokenizer(test_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "outputs = model(**test_encodings)\n",
        "predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "print(\"예측 결과:\", predictions)\n"
      ],
      "metadata": {
        "id": "MSvisn8uJ4q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3OizpS1hJ4ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X_VpLQmXJ4mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KoRoBERTa\n",
        "- RoBERTa가 영어 학습에 최적화되어있기 때문에 성능이 영어 데이터를 사용할 때보다 더 낮게 나타날 수 밖에 없다고 생각했다.\n",
        "- 한국어 자연어 처리(NLP)를 위해 한국어 데이터로 학습된 RoBERTa 모델이다.\n",
        "    - 한국어 문법과 어휘의 특성을 반영하도록 대규모 한국어 데이터로 사전 학습되었다.\n",
        "\n",
        "- `koichi-san/korean-roberta-base`\n",
        "    - 다른 모델 `KLUE-RoBERTa`나 `KoBERT` 등과 비교했을 때 이 모델은 중간 크기의 모델로, 학습 속도와 메모리 사용 측면에서 효율적이다.\n",
        "    - 다양한 한국어 데이터(일상적인 대화, 뉴스기사 등)를 사용하여 사전 학습되었기 때문에 도메인 의존성이 크지 않고 여러 분야의 텍스트에 적합하게 사용할 수 있다.\n",
        "    "
      ],
      "metadata": {
        "id": "s9zLAhbSbKvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 불러오기\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. 데이터 로드\n",
        "data = pd.read_csv('/content/drive/MyDrive/BITAMIN/24 nlp [projects]/주제별 카톡(1)/data.csv')\n",
        "\n",
        "# 변수명 'text'와 'label'로 설정\n",
        "texts = data['text'].values\n",
        "labels = data['subject'].values\n",
        "\n",
        "# 2. 레이블을 숫자로 변환 (예: \"건강\" -> 0, \"여행\" -> 1, ...)\n",
        "label_mapping = {label: idx for idx, label in enumerate(set(labels))}\n",
        "labels = [label_mapping[label] for label in labels]\n",
        "\n",
        "# 3. 데이터셋 분할\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. KoRoBERTa 토크나이저와 모델 불러오기\n",
        "model_name = \"korca/koroberta-base-lkm\" # 원래는 koichi-san/korean-roberta-base\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "num_labels = len(label_mapping)  # 주제의 개수에 따라 num_labels 설정\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "# 5. 토큰화\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# 6. 데이터셋 준비\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = TextDataset(train_encodings, train_labels)\n",
        "val_dataset = TextDataset(val_encodings, val_labels)\n",
        "\n",
        "# 7. 학습 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # 출력 폴더\n",
        "    num_train_epochs=3,              # 학습 반복 횟수\n",
        "    per_device_train_batch_size=8,   # 학습 배치 크기\n",
        "    per_device_eval_batch_size=8,    # 평가 배치 크기\n",
        "    warmup_steps=500,                # 워밍업 단계\n",
        "    weight_decay=0.01,               # 가중치 감쇠\n",
        "    logging_dir='./logs',            # 로깅 폴더\n",
        "    evaluation_strategy=\"epoch\",     # 각 에포크마다 평가\n",
        "    report_to=\"none\"                 # W&B 비활성화\n",
        ")\n",
        "\n",
        "# 8. Trainer 설정\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "# 9. 모델 학습\n",
        "trainer.train()\n",
        "\n",
        "# 10. 평가\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"평가 결과:\", eval_results)\n",
        "\n",
        "# 11. 새로운 텍스트 예측\n",
        "test_text = [\"이것은 예시 문장입니다.\"]\n",
        "test_encodings = tokenizer(test_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "outputs = model(**test_encodings)\n",
        "predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "print(\"예측 결과:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "N1eTE9xKJ4j1",
        "outputId": "42cfe36a-9957-4f8e-8d33-89af26d0b88e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at korca/koroberta-base-lkm and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 05:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.547700</td>\n",
              "      <td>0.622893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.453600</td>\n",
              "      <td>0.603380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.186400</td>\n",
              "      <td>0.513530</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "평가 결과: {'eval_loss': 0.5135300159454346, 'eval_runtime': 7.1673, 'eval_samples_per_second': 139.522, 'eval_steps_per_second': 17.44, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-61e49ec67e2d>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mtest_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"이것은 예시 문장입니다.\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mtest_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtest_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"예측 결과:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1196\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터와 모델이 서로 다른 장치(CPU와 GPU)에 있어서 발생한 오류\n",
        "# 이 문제를 해결하려면 모델과 입력 데이터 모두를 동일한 장치(GPU 또는 CPU)로 이동시켜야 한다는데...\n",
        "\n",
        "# CPU로 옮긴...\n",
        "import torch\n",
        "\n",
        "# 현재 장치를 CPU로 설정\n",
        "device = torch.device(\"cpu\")  # GPU 대신 CPU로 강제 설정\n",
        "\n",
        "# 모델을 CPU로 이동\n",
        "model.to(device)  # 모델을 CPU에 위치시킵니다.\n",
        "\n",
        "# 예측 부분\n",
        "test_text = [\"이것은 예시 문장입니다.\"]\n",
        "test_encodings = tokenizer(test_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "test_encodings = {key: val.to(device) for key, val in test_encodings.items()}  # 입력 데이터를 CPU로 이동\n",
        "\n",
        "# 예측 실행\n",
        "outputs = model(**test_encodings)\n",
        "predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "print(\"예측 결과:\", predictions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWdLBrZb4fWG",
        "outputId": "0b47c513-0934-4292-a468-3474e4e03908"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측 결과: tensor([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 결과 해석하기\n",
        "\n",
        "# 기존의 label_mapping을 뒤집어 reverse_label_mapping을 생성\n",
        "reverse_label_mapping = {idx: label for label, idx in label_mapping.items()}\n",
        "\n",
        "# 예측 결과 해석\n",
        "prediction_label = predictions.item()  # tensor([4])에서 숫자 4를 추출\n",
        "predicted_topic = reverse_label_mapping[prediction_label]  # 숫자 4에 해당하는 주제 이름 찾기\n",
        "\n",
        "print(\"예측된 주제:\", predicted_topic)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dzaU0Fw7G8L",
        "outputId": "59df9203-cee1-41b4-ae7b-1e442c3763ff"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "예측된 주제: 사회이슈\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KoBERTa를 주제 생성에 그대로 사용할 수 없는 이유\n",
        "KoBERTa는 **인코더 기반의 모델**로, 텍스트를 입력 받아 문맥을 이해하고 의미를 파악하는 데 적합하다. 하지만 KoBERTa는 생성 모델이 아니므로, 주제 생성 작업에는 한계가 있다.\n",
        "\n",
        "- 잠깐 생각한건\n",
        "    - KoBERTa로 텍스트 요약을 한다. 이걸 training data로 사용하고\n",
        "    - 이걸 t5에 학습시킨다?\n",
        "\n",
        "- 대화 주제 생성을 할 수 있는 KoBART를 사용해보자!"
      ],
      "metadata": {
        "id": "isFb8PqEjBIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KoBART\n",
        ": 한국어 텍스트 생성에 최적화된 BART 모델   \n",
        "(BART = BERT + GPT)\n",
        "- 인코더-디코더 구조  \n",
        ":인코더-디코더 구조를 사용하여 텍스트를 입력받아 문맥을 이해한 후, 디코더가 이를 기반으로 새로운 텍스트를 생성한다. 이는 텍스트 요약이나 주제 생성처럼, 기존 정보를 요약하거나 압축하여 간단한 형태로 만들어야 하는 작업에서 강력한 성능을 보인다."
      ],
      "metadata": {
        "id": "M4gg7MIlkfxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. KoBART 모델과 토크나이저 불러오기\n",
        "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
        "\n",
        "model_name = \"gogamza/kobart-base-v2\"\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# 2. 주제 생성 함수 정의\n",
        "def generate_topic(text):\n",
        "    # 입력 텍스트 토큰화\n",
        "    inputs = tokenizer([text], max_length=1024, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    # KoBART 모델을 통해 주제 생성\n",
        "    output_ids = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        num_beams=5,                 # 빔 수를 높여 다양성 확보\n",
        "        max_length=50,               # 출력될 최대 토큰 길이\n",
        "        early_stopping=True,         # 의미 있는 지점에서 멈춤\n",
        "        no_repeat_ngram_size=2,      # 반복 방지를 위해 n-gram 반복 금지\n",
        "        do_sample=True,              # 샘플링 모드 활성화\n",
        "        temperature=0.7,             # 온도를 설정하여 결과 다양성 제어\n",
        "        top_k=50,                    # 상위 k개의 단어에서 선택\n",
        "        top_p=0.92                   # 누적 확률 0.92에 해당하는 단어들 중에서 선택\n",
        "    )\n",
        "\n",
        "    # 생성된 텍스트 디코딩\n",
        "    topic = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return topic\n",
        "\n",
        "# 3. 주제 생성 테스트\n",
        "test_text = data['text'][0]\n",
        "predicted_topic = generate_topic(test_text)\n",
        "print(\"생성된 주제:\", predicted_topic)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXaDQC8pkdKq",
        "outputId": "361aa614-c9fd-47d8-ee1b-391d47064ad2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 주제: 1 : 그냥 지구가 난리다  \n",
            "3 : 인도는 요새 계속 난리네?\n",
            "1 : 사람이 벌써 30명이 죽었대!\n",
            "2 : 우리나라도 그렇고 하하\n",
            " 3 : 헐 대박이네\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ZiIlKofYkdEH",
        "outputId": "61ba23d6-3804-4bd9-c7a5-892745228bdc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1 : 인도는 폭우가 와서 난리라네\\n2 : 요새 그냥 지구가 난리다 ㅠㅠ\\n3 : 인도는 요새 계속 난리네 ?\\n1 : 사람이 벌써 30명이 죽었대!\\n2 : 우리나라도 그렇고 하하\\n3 : 헐 대박이네\\n1 : 홍수가 나가꾸 ㅠ 난리 난리\\n2 : 요새 왜이리 이상기후 심한 거 같니\\n3 : 얼마나 났길 래 사람이죽어\\n1 : 먼 사람이 그리 많이 죽었을꼬\\n2 : 이러다가 진짜 멸망하는 거 아니냐고\\n3 : 맞아 ㅜㅜ 너무 날씨 미쳤음\\n1 : 거기 홍수 나면 답도 없을 듯\\n2 : 욜로족으로 살아야 되나\\n3 : 우리 살아 있을 때 멸망 안 하길\\n1 : 엉 지구가 아파요다 완전\\n2 : 플라스틱을 줄이라고 하하\\n3 : 그래 ㅜㅜ\\n3 : 우리도 그렇고 다들 그렇때\\n1 : 근데 옛날에도 홍수는 낫지 않아 ?\\n2 : 마지막으로 지구가 주는 경고야 이건\\n3 : 맞지 ㅜㅜ플라스틱진짜 미쳤다\\n3 : 요새 배달도그렇고\\n1 : 지구 온난화가 영향이 있는 건가?\\n2 : 구니까 우리하나하나가 노력해야함\\n3 : 그렇지 않을까 키키'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3owCEo3pkdAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCNywFn2kc9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dz7rnvdrkcys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L65RZjb8J4Z5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}